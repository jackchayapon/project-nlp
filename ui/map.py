# ui/map.py
import streamlit as st
import folium
from streamlit_folium import st_folium
import psycopg2
import pandas as pd
import os
from datetime import datetime
import json
from .location_data import LOCATION_COORDINATES # Relative import for module within the same package
from folium.plugins import MarkerCluster, MiniMap
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Database connection to external Neon database
DB_URI = "postgresql://neondb_owner:npg_o6AY4XRynVZK@ep-wandering-grass-a1ha7u59-pooler.ap-southeast-1.aws.neon.tech/neondb?sslmode=require"

def get_db_connection():
    """Get database connection to external Neon database"""
    try:
        return psycopg2.connect(DB_URI)
    except Exception as e:
        logger.error(f"Database connection error: {e}")
        st.error(f"à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¸à¸²à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥: {e}")
        return None

@st.cache_data(ttl=1800)
def fetch_all_news_for_risk_assessment():
    """à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¹ˆà¸²à¸§à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸—à¸µà¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡"""
    conn = get_db_connection()
    if not conn:
        return pd.DataFrame()

    try:
        cursor = conn.cursor()
        query = """
            SELECT
                id,
                title_th, title_en, title_ko, title_jp,
                content_raw,
                content_translated_th, content_translated_en, content_translated_ko, content_translated_jp,
                date,
                source,
                url,
                summary_th, summary_en, summary_ko, summary_jp,
                hashtags_th, hashtags_en, hashtags_ko, hashtags_jp
            FROM epidemic_news
            ORDER BY date DESC;
        """
        cursor.execute(query)
        rows = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
        news_df = pd.DataFrame(rows, columns=columns)
        return news_df
    except Exception as e:
        logger.error(f"Error fetching all news data for risk assessment: {e}")
        st.error(f"à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¹ˆà¸²à¸§à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡: {e}")
        return pd.DataFrame()
    finally:
        if conn:
            cursor.close()
            conn.close()

def parse_hashtags(value):
    """Parses hashtag strings from various formats into a list."""
    if isinstance(value, (list, tuple)):
        return list(value)
    if isinstance(value, str):
        try:
            parsed = json.loads(value)
            if isinstance(parsed, list):
                return parsed
        except json.JSONDecodeError:
            if value.startswith('{') and value.endswith('}'):
                tags = value.strip('{}').split(',')
                return [tag.strip().strip('"') for tag in tags if tag.strip()]
    return []

def extract_province_risk_from_content(news_df, current_lang='th'):
    """à¸”à¸¶à¸‡à¸£à¸°à¸”à¸±à¸šà¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¸•à¸²à¸¡à¸ˆà¸±à¸‡à¸«à¸§à¸±à¸”à¹ƒà¸™à¸›à¸£à¸°à¹€à¸—à¸¨à¹„à¸—à¸¢à¹‚à¸”à¸¢à¸­à¸´à¸‡à¸ˆà¸²à¸à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¹à¸¥à¸°à¹à¸®à¸Šà¹à¸—à¹‡à¸"""
    province_risk = {}

    all_content_fields = ['content_raw', 'content_translated_th', 'content_translated_en', 'content_translated_ko', 'content_translated_jp']
    all_hashtag_fields = ['hashtags_th', 'hashtags_en', 'hashtags_ko', 'hashtags_jp']

    for _, row in news_df.iterrows():
        matched_provinces = set()
        
        full_text_to_search = " ".join([str(row.get(field, '')) for field in all_content_fields]).lower()

        for location_name in LOCATION_COORDINATES:
            if location_name.lower() in full_text_to_search:
                matched_provinces.add(location_name)

        for hashtag_field in all_hashtag_fields:
            hashtags = parse_hashtags(row.get(hashtag_field))
            for tag in hashtags:
                tag_clean = str(tag).lower().strip()
                for location_name in LOCATION_COORDINATES:
                    if tag_clean == location_name.lower():
                        matched_provinces.add(location_name)

        for matched_province in matched_provinces:
            if matched_province not in province_risk:
                province_risk[matched_province] = {
                    'lat': LOCATION_COORDINATES[matched_province]['lat'],
                    'lng': LOCATION_COORDINATES[matched_province]['lng'],
                    'news_count': 0,
                    'news_items': [],
                    'risk_keywords': set()
                }

            province_risk[matched_province]['news_count'] += 1
            
            news_item_for_popup = {
                'id': row.get('id'),
                'title': row.get(f'title_{current_lang}') or row.get('title_th'),
                'summary': row.get(f'summary_{current_lang}') or row.get('summary_th'),
                'date': row.get('date'),
                'source': row.get('source'),
                'url': row.get('url')
            }
            if not any(d['id'] == news_item_for_popup['id'] for d in province_risk[matched_province]['news_items']):
                province_risk[matched_province]['news_items'].append(news_item_for_popup)

            for hashtag_field in all_hashtag_fields:
                hashtags_list = parse_hashtags(row.get(hashtag_field))
                if hashtags_list:
                    province_risk[matched_province]['risk_keywords'].update(hashtags_list)

    for province, data in province_risk.items():
        data['news_items'].sort(key=lambda x: x['date'], reverse=True)

    return province_risk

def create_enhanced_risk_map(province_risk_data, lang='th'):
    """à¸ªà¸£à¹‰à¸²à¸‡à¹à¸œà¸™à¸—à¸µà¹ˆà¹à¸šà¸šà¹‚à¸•à¹‰à¸•à¸­à¸šà¸žà¸£à¹‰à¸­à¸¡à¸•à¸±à¸§à¸šà¹ˆà¸‡à¸Šà¸µà¹‰à¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¸•à¸²à¸¡à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸•à¸³à¹à¸«à¸™à¹ˆà¸‡"""
    center_lat, center_lng = 13.7563, 100.5018
    m = folium.Map(location=[center_lat, center_lng], zoom_start=6, tiles='CartoDB positron')

    labels = {
        'th': {'risk_level': 'à¸£à¸°à¸”à¸±à¸šà¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡', 'news_count': 'à¸ˆà¸³à¸™à¸§à¸™à¸‚à¹ˆà¸²à¸§', 'latest_news': 'à¸‚à¹ˆà¸²à¸§à¸¥à¹ˆà¸²à¸ªà¸¸à¸”', 'related_hashtags': 'Hashtag à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡', 'high': 'à¸ªà¸¹à¸‡', 'medium': 'à¸›à¸²à¸™à¸à¸¥à¸²à¸‡', 'low': 'à¸•à¹ˆà¸³'},
        'en': {'risk_level': 'Risk Level', 'news_count': 'News Count', 'latest_news': 'Latest News', 'related_hashtags': 'Related Hashtags', 'high': 'High', 'medium': 'Medium', 'low': 'Low'},
        'ko': {'risk_level': 'ìœ„í—˜ ìˆ˜ì¤€', 'news_count': 'ë‰´ìŠ¤ ìˆ˜', 'latest_news': 'ìµœì‹  ë‰´ìŠ¤', 'related_hashtags': 'ê´€ë ¨ í•´ì‹œíƒœê·¸', 'high': 'ë†’ìŒ', 'medium': 'ì¤‘ê°„', 'low': 'ë‚®ìŒ'},
        'jp': {'risk_level': 'ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«', 'news_count': 'ãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°', 'latest_news': 'æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'related_hashtags': 'é–¢é€£ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°', 'high': 'é«˜', 'medium': 'ä¸­', 'low': 'ä½Ž'}
    }
    current_labels = labels.get(lang, labels['th'])

    marker_cluster = MarkerCluster().add_to(m)

    for province, risk_data in province_risk_data.items():
        news_count = risk_data['news_count']
        if news_count >= 5:
            risk_level, marker_color = 'high', '#dc3545'
        elif news_count >= 2:
            risk_level, marker_color = 'medium', '#ffc107'
        else:
            risk_level, marker_color = 'low', '#28a745'

        latest_news = risk_data['news_items'][:3]
        news_html = ""
        for news in latest_news:
            title = news.get('title', 'N/A')
            summary = news.get('summary', '')
            date_str = news['date'].strftime('%Y-%m-%d') if hasattr(news['date'], 'strftime') else 'N/A'
            source_link = f"<a href='{news['url']}' target='_blank' style='color: #007bff;'>{news.get('source', 'N/A')}</a>" if news.get('url') else news.get('source', 'N/A')
            news_html += f"<b>{title}</b><br><small>{summary[:100]}...</small><br><small>({date_str} | {source_link})</small><hr style='margin: 5px 0;'>"
        
        if not news_html:
            news_html = "<p>à¹„à¸¡à¹ˆà¸žà¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¹ˆà¸²à¸§</p>"

        hashtags = list(risk_data.get('risk_keywords', set()))[:7]
        hashtags_html = "".join(f"<span style='display: inline-block; background-color: #e0f2f7; color: #007bff; padding: 3px 8px; border-radius: 12px; font-size: 0.8em; margin: 2px;'>#{tag}</span>" for tag in hashtags if tag)
        if not hashtags_html:
            hashtags_html = "<p>à¹„à¸¡à¹ˆà¸¡à¸µ Hashtag</p>"

        popup_content = f"""
        <div style="width: 300px; font-family: 'Inter', sans-serif;">
            <h4>{province}</h4>
            <p><strong>{current_labels['risk_level']}:</strong> {current_labels[risk_level]} ({news_count} {current_labels['news_count']})</p>
            <hr>
            <p><strong>{current_labels['latest_news']}:</strong></p>
            <div style="font-size: 0.9em; max-height: 150px; overflow-y: auto;">{news_html}</div>
            <hr>
            <p><strong>{current_labels['related_hashtags']}:</strong></p>
            <div style="max-height: 80px; overflow-y: auto;">{hashtags_html}</div>
        </div>
        """
        tooltip_text = f"{province} - {current_labels['risk_level']}: {current_labels[risk_level]}"

        folium.CircleMarker(
            location=[risk_data['lat'], risk_data['lng']],
            radius=6 + (news_count * 1.5),
            popup=folium.Popup(popup_content, max_width=350),
            color=marker_color,
            fill=True,
            fillColor=marker_color,
            fillOpacity=0.7,
            weight=1,
            tooltip=tooltip_text
        ).add_to(marker_cluster)

    MiniMap().add_to(m)
    folium.LayerControl().add_to(m)
    return m

def show():
    """à¹à¸ªà¸”à¸‡à¸«à¸™à¹‰à¸²à¹à¸œà¸™à¸—à¸µà¹ˆà¸žà¸£à¹‰à¸­à¸¡à¸à¸²à¸£à¹à¸ªà¸”à¸‡à¸ à¸²à¸žà¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸£à¸±à¸šà¸à¸²à¸£à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡"""
    lang = st.session_state.get('language', 'th')

    st.title({"th": "ðŸ—ºï¸ à¹à¸œà¸™à¸—à¸µà¹ˆà¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡", "en": "ðŸ—ºï¸ Risk Map", "ko": "ðŸ—ºï¸ ìœ„í—˜ ì§€ë„", "jp": "ðŸ—ºï¸ ãƒªã‚¹ã‚¯ãƒžãƒƒãƒ—"}[lang])
    st.markdown({"th": "à¹à¸œà¸™à¸—à¸µà¹ˆà¹à¸ªà¸”à¸‡à¸£à¸°à¸”à¸±à¸šà¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¸‚à¸­à¸‡à¸à¸²à¸£à¸£à¸°à¸šà¸²à¸”à¹‚à¸£à¸„à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¸ˆà¸±à¸‡à¸«à¸§à¸±à¸”à¸‚à¸­à¸‡à¸›à¸£à¸°à¹€à¸—à¸¨à¹„à¸—à¸¢", "en": "Map showing epidemic risk levels in each province of Thailand", "ko": "íƒœêµ­ ê° ì£¼ì˜ ì „ì—¼ë³‘ ìœ„í—˜ ìˆ˜ì¤€ì„ ë³´ì—¬ì£¼ëŠ” ì§€ë„", "jp": "ã‚¿ã‚¤ã®å„çœŒã®æ„ŸæŸ“ç—‡ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã‚’ç¤ºã™ãƒžãƒƒãƒ—"}[lang])

    news_df_for_risk = fetch_all_news_for_risk_assessment()

    if news_df_for_risk.empty:
        st.warning({"th": "à¹„à¸¡à¹ˆà¸žà¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¹ˆà¸²à¸§à¸ªà¸²à¸£à¸ªà¸³à¸«à¸£à¸±à¸šà¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡", "en": "No news data found for risk assessment", "ko": "ìœ„í—˜ í‰ê°€ë¥¼ ìœ„í•œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "jp": "ãƒªã‚¹ã‚¯è©•ä¾¡ã®ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}[lang])
        return

    province_risk_data = extract_province_risk_from_content(news_df_for_risk, lang)

    if not province_risk_data:
        st.info({"th": "à¹„à¸¡à¹ˆà¸žà¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¸ˆà¸²à¸à¸‚à¹ˆà¸²à¸§à¸ªà¸²à¸£à¸—à¸µà¹ˆà¸£à¸°à¸šà¸¸à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡", "en": "No risk data found from location mentions in news", "ko": "ë‰´ìŠ¤ì—ì„œ ìœ„ì¹˜ ì–¸ê¸‰ìœ¼ë¡œ ì¸í•œ ìœ„í—˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "jp": "ãƒ‹ãƒ¥ãƒ¼ã‚¹å†…ã®ä½ç½®æƒ…å ±ã‹ã‚‰ãƒªã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}[lang])
        return

    risk_map = create_enhanced_risk_map(province_risk_data, lang)
    
    # --- START: à¹‚à¸„à¹‰à¸”à¸—à¸µà¹ˆà¹à¸à¹‰à¹„à¸‚ ---
    # à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¸ˆà¸²à¸ width=700 à¹€à¸›à¹‡à¸™ use_container_width=True à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¹à¸œà¸™à¸—à¸µà¹ˆà¸‚à¸¢à¸²à¸¢à¹€à¸•à¹‡à¸¡à¸„à¸§à¸²à¸¡à¸à¸§à¹‰à¸²à¸‡
    st_folium(risk_map, use_container_width=True, height=500)
    # --- END: à¹‚à¸„à¹‰à¸”à¸—à¸µà¹ˆà¹à¸à¹‰à¹„à¸‚ ---

    st.markdown("---")
    st.subheader({"th": "ðŸ“Š à¸ªà¸–à¸´à¸•à¸´à¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡", "en": "ðŸ“Š Risk Statistics", "ko": "ðŸ“Š ìœ„í—˜ í†µê³„", "jp": "ðŸ“Š ãƒªã‚¹ã‚¯çµ±è¨ˆ"}[lang])

    col1, col2, col3 = st.columns(3)
    high_risk = sum(1 for data in province_risk_data.values() if data['news_count'] >= 5)
    medium_risk = sum(1 for data in province_risk_data.values() if 2 <= data['news_count'] < 5)
    low_risk = sum(1 for data in province_risk_data.values() if data['news_count'] < 2)

    with col1:
        st.metric({"th": "à¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¸ªà¸¹à¸‡", "en": "High Risk", "ko": "ë†’ì€ ìœ„í—˜", "jp": "é«˜ãƒªã‚¹ã‚¯"}[lang], high_risk)
    with col2:
        st.metric({"th": "à¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¸›à¸²à¸™à¸à¸¥à¸²à¸‡", "en": "Medium Risk", "ko": "ì¤‘ê°„ ìœ„í—˜", "jp": "ä¸­ãƒªã‚¹ã‚¯"}[lang], medium_risk)
    with col3:
        st.metric({"th": "à¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¸•à¹ˆà¸³", "en": "Low Risk", "ko": "ë‚®ì€ ìœ„í—˜", "jp": "ä½Žãƒªã‚¹ã‚¯"}[lang], low_risk)