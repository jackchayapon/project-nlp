import requests
from bs4 import BeautifulSoup
import json
import os
import re
import time
from datetime import datetime

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”¹ à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸Šà¹ˆà¸§à¸¢
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean(text: str) -> str:
    """à¸¥à¸šà¸Šà¹ˆà¸­à¸‡à¸§à¹ˆà¸²à¸‡à¹€à¸à¸´à¸™à¹à¸¥à¸°à¸•à¸±à¸”à¸Šà¹ˆà¸­à¸‡à¸§à¹ˆà¸²à¸‡à¸«à¸±à¸§â€“à¸—à¹‰à¸²à¸¢"""
    return re.sub(r'\s+', ' ', text).strip()

def load_existing_urls(path: str) -> set:
    """à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œ JSON à¸—à¸µÂ­Â­à¹ˆà¹€à¸„à¸¢à¹€à¸‹à¸Ÿà¹„à¸§à¹‰ à¹à¸¥à¹‰à¸§à¸”à¸¶à¸‡ URL à¹€à¸à¸·à¹ˆà¸­à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸‚à¹ˆà¸²à¸§à¸‹à¹‰à¸³"""
    if not os.path.exists(path):
        return set()
    try:
        with open(path, encoding="utf-8") as f:
            return {art["url"] for art in json.load(f)}
    except Exception:
        return set()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”¹ à¸”à¸¶à¸‡à¹€à¸™à¸·à¹‰à¸­à¸«à¸²/à¸§à¸±à¸™à¸—à¸µà¹ˆà¸ˆà¸²à¸à¸«à¸™à¹‰à¸²à¹€à¸”à¸µà¹ˆà¸¢à¸§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_article_content_and_date(article_url: str) -> tuple[str, str]:
    try:
        print(f"    â€¢ à¸”à¸¶à¸‡: {article_url}")
        res = requests.get(article_url, headers={"User-Agent": "Mozilla/5.0"}, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        # à¸§à¸±à¸™à¸—à¸µà¹ˆ
        date_tag = soup.select_one("span.field-content")
        date = clean(date_tag.text) if date_tag else ""

        # à¹€à¸™à¸·à¹‰à¸­à¸«à¸²
        content_div = soup.select_one("article div.field--name-body")
        if content_div:
            raw = content_div.get_text(" ", strip=True)
            content = clean(raw)
            print(f"      âœ… à¹€à¸™à¸·à¹‰à¸­à¸«à¸² {len(content)} à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£")
        else:
            content = ""
            print("      â€¢ à¹„à¸¡à¹ˆà¸à¸šà¹€à¸™à¸·à¹‰à¸­à¸«à¸²")

        return date, content
    except Exception as err:
        print(f"âŒ Error @ {article_url} : {err}")
        return "", ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”¹ à¸”à¸¶à¸‡à¸‚à¹ˆà¸²à¸§à¸ˆà¸²à¸à¸«à¸™à¹‰à¸² list
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scrape_hfocus_articles(pages: int = 1, *, existing_urls: set = set()) -> list[dict]:
    base = "https://www.hfocus.org/topics/à¹‚à¸£à¸„à¸­à¸¸à¸šà¸±à¸•à¸´à¹ƒà¸«à¸¡à¹ˆà¸­à¸¸à¸šà¸±à¸•à¸´à¸‹à¹‰à¸³?page={}"
    articles = []

    for page in range(pages):
        url = base.format(page)
        print(f"\nğŸ” à¸«à¸™à¹‰à¸² {page+1}: {url}")
        try:
            res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=15)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")

            links = soup.select("div.views-field-title h3.field-content a")
            if not links:
                print("    â€¢ à¹„à¸¡à¹ˆà¸à¸šà¸‚à¹ˆà¸²à¸§à¸«à¸£à¸·à¸­ selector à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™")
                break

            for a in links:
                link = "https://www.hfocus.org" + a["href"]
                if link in existing_urls:
                    print(f"      â€¢ à¸‚à¹‰à¸²à¸¡à¸‚à¹ˆà¸²à¸§à¸‹à¹‰à¸³: {link}")
                    continue

                title = clean(a.get_text())
                date, content_raw = get_article_content_and_date(link)

                articles.append({
                    "source": "hfocus",
                    "title": title,
                    "url": link,
                    "date": date,
                    "content_raw": content_raw,   # âœ… à¹ƒà¸Šà¹‰ content_raw
                    "content_translated": None,
                    "summary": None,
                    "language": "th",
                    "is_translated": False,
                    "is_summarized": False
                })

                time.sleep(0.5)  # à¸–à¸™à¸­à¸¡à¹€à¸‹à¸´à¸£à¹Œà¸Ÿà¹€à¸§à¸­à¸£à¹Œ

        except requests.exceptions.RequestException as http_err:
            print(f"âš ï¸ HTTP Error: {http_err}")
        except Exception as err:
            print(f"âš ï¸ Error: {err}")

        time.sleep(2)

    return articles

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”¹ Main: à¹ƒà¸Šà¹‰à¸”à¸¶à¸‡à¹à¸¥à¹‰à¸§à¹€à¸‹à¸Ÿà¹€à¸›à¹‡à¸™à¹„à¸Ÿà¸¥à¹Œ JSON
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    print("ğŸš€ à¹€à¸£à¸´à¹ˆà¸¡à¸”à¸¶à¸‡à¸‚à¹ˆà¸²à¸§à¸ˆà¸²à¸ Hfocus.org â€¦")

    today = datetime.now().strftime("%Y%m%d")
    save_dir = "data/raw_news"
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, f"hfocus_news_{today}.json")

    existing = load_existing_urls(save_path)
    new_articles = scrape_hfocus_articles(pages=20, existing_urls=existing)

    print(f"\nâœ… à¸‚à¹ˆà¸²à¸§à¹ƒà¸«à¸¡à¹ˆ {len(new_articles)} à¸£à¸²à¸¢à¸à¸²à¸£")

    # à¸£à¸§à¸¡à¸à¸±à¸šà¸‚à¹ˆà¸²à¸§à¹€à¸à¹ˆà¸²à¸–à¹‰à¸²à¸¡à¸µ
    all_articles = new_articles
    if os.path.exists(save_path):
        with open(save_path, encoding="utf-8") as f:
            all_articles = json.load(f) + new_articles

    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(all_articles, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“ à¸šà¸±à¸™à¸—à¸¶à¸à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” {len(all_articles)} à¸‚à¹ˆà¸²à¸§ â†’ {save_path}")
