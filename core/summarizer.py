# summarizer.py

from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from typing import Literal
import re
from pythainlp.tokenize import sent_tokenize

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIG: Load summarization model (English only)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

model_name = "facebook/bart-large-cnn"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
summarizer_pipeline = pipeline("summarization", model=model, tokenizer=tokenizer)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UTIL: à¹à¸šà¹ˆà¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸¢à¸²à¸§à¹€à¸›à¹‡à¸™ Chunk à¸•à¸²à¸¡à¸¢à¹ˆà¸­à¸«à¸™à¹‰à¸²
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def split_into_chunks(text: str, max_chunk_chars: int = 1000):
    """
    à¹à¸šà¹ˆà¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸•à¸²à¸¡à¸¢à¹ˆà¸­à¸«à¸™à¹‰à¸²à¸«à¸£à¸·à¸­à¸ˆà¸¸à¸” à¸ˆà¸™à¸à¸§à¹ˆà¸²à¸ˆà¸°à¸„à¸£à¸šà¸„à¸§à¸²à¸¡à¸¢à¸²à¸§ max_chunk_chars
    """
    paragraphs = re.split(r'(?<=[.!?])\s+', text)
    chunks, current_chunk = [], ""

    for para in paragraphs:
        if len(current_chunk) + len(para) <= max_chunk_chars:
            current_chunk += " " + para
        else:
            chunks.append(current_chunk.strip())
            current_chunk = para
    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SUMMARIZER: à¸ à¸²à¸©à¸²à¸­à¸±à¸‡à¸à¸¤à¸©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def summarize_en(text: str, max_length=150, min_length=30) -> str:
    if not text.strip():
        return ""
    
    chunks = split_into_chunks(text, max_chunk_chars=1000)
    summaries = []

    for i, chunk in enumerate(chunks):
        print(f"ğŸ§  à¸ªà¸£à¸¸à¸› Chunk {i+1}/{len(chunks)} â€¦")
        try:
            word_count = len(chunk.split())
            # à¸›à¸£à¸±à¸š max/min à¸•à¸²à¸¡à¸„à¸§à¸²à¸¡à¸¢à¸²à¸§à¸ˆà¸£à¸´à¸‡
            dynamic_max = min(max_length, max(30, int(word_count * 0.7)))
            dynamic_min = min(min_length, max(10, int(word_count * 0.3)))

            if dynamic_min >= dynamic_max:
                dynamic_min = max(5, dynamic_max - 5)

            summary = summarizer_pipeline(
                chunk,
                max_length=dynamic_max,
                min_length=dynamic_min,
                do_sample=False
            )
            summaries.append(summary[0]['summary_text'].strip())
        except Exception as e:
            summaries.append(f"[ERROR] {e}")

    return " ".join(summaries)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SUMMARIZER: à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ (à¸•à¸±à¸”à¸›à¸£à¸°à¹‚à¸¢à¸„)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def summarize_th(text: str, max_sentences=3) -> str:
    if not text.strip():
        return ""
    sentences = sent_tokenize(text)
    return " ".join(sentences[:max_sentences])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UNIVERSAL ENTRY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def summarize(text: str, lang: Literal["en", "th", "ko"] = "en") -> str:
    """
    à¹ƒà¸Šà¹‰ summarize_en à¸ªà¸³à¸«à¸£à¸±à¸š en/ko (à¸œà¹ˆà¸²à¸™ Pivot à¹à¸¥à¹‰à¸§),
    à¹ƒà¸Šà¹‰à¸•à¸±à¸”à¸›à¸£à¸°à¹‚à¸¢à¸„à¸ªà¸³à¸«à¸£à¸±à¸š th
    """
    if lang == "en" or lang == "ko":
        return summarize_en(text)
    elif lang == "th":
        return summarize_th(text)
    else:
        return "[ERROR] Unsupported language"
    
print(f"âœ… à¹ƒà¸Šà¹‰ device: {model.device}")

