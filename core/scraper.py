import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import os
import re

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TH_MONTH = {
    'à¸¡à¸à¸£à¸²à¸„à¸¡': '01', 'à¸à¸¸à¸¡à¸ à¸²à¸à¸±à¸™à¸˜à¹Œ': '02', 'à¸¡à¸µà¸™à¸²à¸„à¸¡': '03',
    'à¹€à¸¡à¸©à¸²à¸¢à¸™': '04', 'à¸à¸¤à¸©à¸ à¸²à¸„à¸¡': '05', 'à¸¡à¸´à¸–à¸¸à¸™à¸²à¸¢à¸™': '06',
    'à¸à¸£à¸à¸à¸²à¸„à¸¡': '07', 'à¸ªà¸´à¸‡à¸«à¸²à¸„à¸¡': '08', 'à¸à¸±à¸™à¸¢à¸²à¸¢à¸™': '09',
    'à¸•à¸¸à¸¥à¸²à¸„à¸¡': '10', 'à¸à¸¤à¸¨à¸ˆà¸´à¸à¸²à¸¢à¸™': '11', 'à¸˜à¸±à¸™à¸§à¸²à¸„à¸¡': '12'
}

def parse_thai_date(text: str) -> str:
    """à¹à¸›à¸¥à¸‡ '17 à¸à¸¤à¸©à¸ à¸²à¸„à¸¡ 2025' âœ '2025-05-17'"""
    parts = text.strip().split()
    if len(parts) == 3 and parts[1] in TH_MONTH:
        day, month_th, year = parts
        return f"{year}-{TH_MONTH[month_th]}-{int(day):02d}"
    return text.strip()

def fetch_article_body(url: str) -> str:
    """à¸”à¸¶à¸‡à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¹€à¸•à¹‡à¸¡à¸ˆà¸²à¸à¹€à¸à¸ˆà¸‚à¹ˆà¸²à¸§"""
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=15)
        soup = BeautifulSoup(res.text, "html.parser")
        body = soup.select_one("div.entry-content")
        if not body:
            return ""
        paras = [p.get_text(" ", strip=True) for p in body.find_all(["p", "li", "blockquote"])]
        return "\n\n".join([re.sub(r'\s+', ' ', p) for p in paras if p])
    except Exception:
        return ""

def load_existing_urls(path: str) -> set:
    """à¹‚à¸«à¸¥à¸” URL à¸—à¸µà¹ˆà¹€à¸„à¸¢à¹€à¸à¹‡à¸šà¹à¸¥à¹‰à¸§ à¹€à¸à¸·à¹ˆà¸­à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‹à¹‰à¸³"""
    if not os.path.exists(path):
        return set()
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            return set(article["url"] for article in data)
    except Exception:
        return set()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main scraper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scrape_standard(max_pages: int = 24, existing_urls: set = set()) -> list[dict]:
    base = "https://thestandard.co/tag/à¹‚à¸£à¸„à¸£à¸°à¸šà¸²à¸”/page/{}/"
    out = []

    for page in range(1, max_pages + 1):
        url = base.format(page)
        print(f"ğŸ“„ page {page}: {url}")
        try:
            res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=15)
            soup = BeautifulSoup(res.text, "html.parser")
            cards = soup.select("div.news-item")
            if not cards:
                break

            for card in cards:
                a = card.select_one("h3.news-title a")
                if not a:
                    continue
                art_url = a["href"]

                if art_url in existing_urls:
                    print(f"   ğŸ” à¸‚à¹‰à¸²à¸¡à¸‚à¹ˆà¸²à¸§à¸‹à¹‰à¸³: {art_url}")
                    continue

                title = a.get_text(strip=True)
                date_raw = card.select_one("div.date")
                date = parse_thai_date(date_raw.get_text(strip=True)) if date_raw else ""
                content_raw = fetch_article_body(art_url)

                out.append({
                    "source": "thestandard",
                    "title": title,
                    "url": art_url,
                    "date": date,
                    "content_raw": content_raw,  # âœ… à¹ƒà¸Šà¹‰ content_raw à¹à¸—à¸™ content
                    "language": "th",
                    "is_translated": False,
                    "is_summarized": False
                })
        except Exception as e:
            print(f"âš ï¸ Error page {page}: {e}")
            continue

    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ runner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    save_path = "data/raw_news/thestandard_news.json"
    existing_urls = load_existing_urls(save_path)

    print("ğŸš€ à¹€à¸£à¸´à¹ˆà¸¡à¸”à¸¶à¸‡à¸‚à¹ˆà¸²à¸§à¸ˆà¸²à¸ The Standard â€¦")
    new_articles = scrape_standard(max_pages=24, existing_urls=existing_urls)

    if os.path.exists(save_path):
        with open(save_path, "r", encoding="utf-8") as f:
            old_articles = json.load(f)
    else:
        old_articles = []

    all_articles = old_articles + new_articles
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(all_articles, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… à¸‚à¹ˆà¸²à¸§à¹ƒà¸«à¸¡à¹ˆ: {len(new_articles)} à¸‚à¹ˆà¸²à¸§")
    print(f"ğŸ“¦ à¸£à¸§à¸¡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: {len(all_articles)} à¸‚à¹ˆà¸²à¸§")
    print(f"ğŸ“ à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸§à¹‰à¸—à¸µà¹ˆ: {save_path}")
